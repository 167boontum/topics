
# Topics API — Breakout, TPAC 2022

September 14th 2022

Josh Karlin: Engineer at Chrome, been on Privacy Sandbox essentially since the beginning — work on Topics and also Shared Storage and Fenced Frames and other stuff.

Agenda: We'll talk about the use case, the design goals, how Topics works, current status, and open issues /points of discussion

Note Taker: Michael Kleber

This is about a subset of ads personalization, where we derive interests from which sites the user has visited in the past.  This is "interest-based advertising", rather than contextual.  If your favorite website is about stamps, then you might love it and visit it a lot and want it to thrive, but there just aren't a lot of ads about stamps.  If the ads ecosystem knows you're also in electric cars, yorus tamps site can be happy.

Browsers need to worry about this because 3p cookies are going away.

Things we need to worry about: Privacy!



* Should be difficult to reidentify users across sites
* WOuld like to say "no sensitive information should be able to be learned", but what is sensitive is too subjective, cannot make that kind of claim.  Can say that any info should be much less sensitive than today
* Should be ideally comprehensible by users
* Should be controllable, by both users and sites

Need to be useful!  Otherwise don't bother launching it



* Easily adoptable — better than many of the other Privacy Sandbox APIs
* Performant
* does a good job of helping make money

Representing interests to sites:



* How to represent user interests? 
    * Lesson from FLoC: Needs to be more clear about what information is being shared!  Otherwise tech press will write about how your topic indicates your favorite ice cream flavor.

Privacy budgeting: What is it adding to ecosystem in terms of user re-identification?



* Ideally it should take on average say 10s of weeks of repeat visits to two sites for them to have some confidence in recognizing the same user on two sites.

How to infer user interests?



* Ideally, sites just state user interests!  That would be great.  But then sites will lie in an attempt to get more money.  That would just pollute the data, make API useless.
    * So the browser needs to be the one that decides what the site's topics are about

Q, Graham: Would that really happen?

Josh: I would love it if we could incentivize doing this right, but it is clearly a concern.

How Topics Works:

Very simple API, document.getBrowsingTopics(), get one topic from each of the past three weeks.  But a lot of complexity below the surface.



1. Sites are mapped to a public list of human-curated, recognizable Topics
2. Each week, a user's browser collects the top topics from sites the user visited that called the API
3. When a user visits a site that calls the API, the user's browser shares a limited set of those top topics, one per site per week.
4. (missed this)

Initial taxonomy has 349 topics.  We expect this to evolve.  It's the intersection of a Google-made context taxonomy and an IAB-made taxonomy.  Ugh.  Why?  DIdn't want to be Google-centric, so wanted someone else's opinions on the taxonomy.  But the Google taxonomy is what the have for all the training data, so that's what we can do an ML model of in the browser.

Topics are derived from hostname, not from content.  This is really hard — "drive.google.com" is obviously a site about searching for cars, right?  No.  The top 10K sites are manually verified with human-assigned topics; this covers 60% of web traffic.  Remaining sites are classified with an ML model.  Independent ad tech report said that we're doing an OK job.

You only get the topics that you, an ad tech, have seen for this user on other pages.  That is, if the user's topic for this week includes both Sports and Food&Drink and three others, and you call the Topics API the next week, then you might get Sports if it's the randomly selected topic if you used the API on the Sports site, but not get Food&Drink if you were not a part that saw this user on a Food&Drink site.

For a given user, site, and week, only one of the topic 5 topics the user visited that week are revealed.  That choice of 1-of-5 is stable for that site the user visited for the whole three weeks.  But on a different site the user visits, it's an independent 1-of-5 choice for each week, making re-identification much harder.

Status:

We're currently at a 1% Origin Trial.  (Usually OTs don't run on just a percentage of people, so Chrome is limiting the fraction of users that have the API available to them.)  Which users have the OT available is the same across all ad tech callers that are experimenting with it.

Positions from other browsers: Webkit was negative, no response yet from Mozilla or Edge.

Martin: I'll give you informal feedback later in this session.

See chrome://topics-internals to see what your browser thinks and to try it out.

Performance questions under discussions now:



1. Topics in HTTP headers (rather than needing to create iframes and encrypt and postMessage etc).
    * Aram Z-S: Agreed — it is a regression if you need us to put scripts and iframes on the page.  We publishers really want to move away from that.
    * Don Marti: You just need 1 iframe, right?
    * Aram: No, there will be 20 iframes all saying "put my iframe on your page to get the best topics".  Only 1 would be optimal but in reality that's not how it would work
    * Martin: If you didn't have the witness requirement (per-domain filtering), this wouldn't be a problem.
2. Filtering per-caller encourages lots of ad-tech on page or else giant topic reflectors.
    * Aram: Lots of incentives to operate badly with per-topic filtering.  Without filtering, would be much easier to verify, so there is still incentive to lie but much harder to get away with.
    * Martin: Also with this requirement, you're working from a baseline of 3p cookies, which no one except Chrome has.  So the "strictly better than 3p cookies" POV is a non-goal.  Something like this is going to be worse for privacy, to some extent.  All you've done by creating the per-caller filtering is perverse incentives in favor of large players on many websites.  There's a significant performance impact for just being a witness is limited, so again encourages more centralization to either large brokers or large existing players like Goog/FB.

Erik: Not sure we (FB) would use this API.  But it seems like this is correcting some problem that existing with cross-site tracking, ability to recognize people across sites.  Once you remove that, there is a shift in who can do things.  From our standpoint, we have a lot of 1p context and would continue to just use that.

Josh: Every site is going to have to make that decision for itself — what am I getting for this, and what am I giving up?

Graham: If you're an ad tech that is also a major O&O property?

Aram: There is actually an incentive to not share!  WaPo would use our own version of topics, but wouldn't want to share that to any other operator.

Nick Doty: But is the design such that you would have to?

Aram: Prebid setup would do that sharing.  If I'm a self-sustained SSP, I can pull that data off of the page, decide how to use it, never share it back.

Josh: This is a really interesting question to me.  As the API is written right now, if you call the API and get topics, then that page's topics are eligible for the global topics — it's a coop, pay-to-play, you can't get a topic without giving your site topic to the system.  To make Headers work, we need to split this up, so it _would_ be possible to get but not set.  Then we're back in a system like today, where it might be pay-to-play but only because of ad techs making their implementation work that way.

Graham: But what about large sites where you only get the domain name?

Josh: I would love to use more information, like say the whole domain name!  But there is a user surprise problem.  There are lots of pages that insert user search query params into the URL.  So now a thing you search for might become a topic.  Now the user is surprised.

Nick: I understand user surprise — that is still a problem no matter what, if you search for something and then click on a link!

Josh: Indeed.  So I'm interested in including URL minus query params.

Don: Raw URL could expose something sensitive.  Could just let caller pass in the subsection of this site.  So a large user-generated content site could indicate which part of the site this page is from.  Right now it's an asymmetric data flow towards larger sites; if large sites played along, then that could make it worthwhile.

Josh: Not what we expect at all.

Aram: Risk of mixing Google business units here, but all sites split up their site into sections already, for Google SEO reasons.  We have section information in URL, in metadata, JSON, etc.  COudl be usable by TOpics API.  Sites will surely lie, yes, but they are probably already lying in lots of other ways too.  Fact that the set of topics is limited means lying won't have that much effect.  Using existing specs to get the topics of this website section means you wouldn't need to ask people to even make a change — existing IAB topic or whatever.

Martin: Problem with giving sites ability to set topics is that it opens the ability for sites to track in interesting ways.  349 topics -> billions of combinations after 3 weeks, easily enough to encode a user identifier.  Not hard for a site to inject stuff into user history to exert control if that's your goal.  So it's a little scary that someone might use it for tracking purposes.  And since a site has no incentive to make good topics for themselves, only for other people, sites could maliciously do this.

Josh: +1, totally agree with that risk.  Also it allows you to change the meaning of a topic!  You could take one innocuous topic, use it as a codeword for something sensitive.

Erik: Capturing the entire URL opens up that same vector

Josh: Indeed.  Even just the domain opens this up a bit, but it is a little harder.  But YouTube isn't contributing — and this wouldn't help with reading the whole URL; a youtube URL is just an encoded ID.  The only one who has incentive is the ad tech.  That's not going to happen on big O&Os — why would Amazon or YouTUbe do it?

Ben Savage: Back to user surprise.  I care a lot about aligning data flows with user expectations.  My bad idea: What if there was some UX where when you visit some page/domain/etc, there is some UX signposting that lets you know when a topic gets added?  Let you glance and see that a topic addition happened, and what topic?  And if someone is gaming it, you might notice.  And if you didn't want to be targeted, you could un-check some box.  The fact the Topics is passive by nature right now, could turn it into some sort of user control.

Josh: As a browser engineer, for every feature I ever make, I think it would be better with some UX, and it is UX's job to tell me no.  And I love them for it, otherwise the UX would just explode.  So I don't want to say no, but it's a high bar.  There are other ways to make things public, e.g. Chrome UX Report, 

Graham: I think this is still something worth considering — burying the way that advertising works is an original sin; making things visible might anyway be the way to bring trust back to the industry.

Camille: Showing what the site is doing there is very limited space.  Training users to look at the URL bar is hard, even showing them that the page has camera access or the like is very hard.  Users will stop paying attention to things.  If this is showing up on every site, they will ignore it.

Erik: THere is risk of offloading the privacy work to the user.

Martin: Eyeo has done work on this, which does what Ben described.  Would be interesting to get thoughts on this.  Not sure how effective it is; I tend to agree with Camille.  I did try to game the classifier using domain names, it's not bad but takes some work.  If you feed it random noise, everything is either News or Books & Literature.

Aram: I think this is an artifact of the IAB Taxonomy, so News is the most neutral term

Alex: Maybe because the taxonomy was created when mostly publishers.

Lionel: From Criteo — I'd like to explain our use case and how it might be solved by topics API.  We have marketers we work with, who say "I have a new sports watch I want to sell, run an ad campaign for that"  We will create a profile of an audience that we might want to show the ad to, e.g. people who have been to some sports sites.  The way we would use the API — We would want to define Topics for those users (and ideally not give them to other buyers!) by calling the TOpics API when the user is on sports pages.  We use only topics we observe or define.

Aram: When it comes to site sections, you could not let the publisher dictate section topics, just let the publisher partition the site, and the existing model needs to pick the topics for each section.  It's just publishers saying "here is a different type of content."

Don: Re user interface, where are you with user research on how people feel about this or react to it?  Especially in contexts that are not ad related?  School, appluing for apartment, etc.

Theo: We have some research that we will publish soon.  If you have some UX showing topic soon after page load, and lets them have control over whether the topic is used or gives them a way to block it, that resonates and is really well received.  The general idea that the user can say "no I don't want this topic" is very strong.  To Ben's point: if users aren't really looking for this information, it will get to visual noise.  But if we can surface this when a user wants to know why they are seeing an ad, this can be really valuable.  Will be published soon.

Erik: What do you use for domain?  (A: Full subdomain gets used, so no added stress on the PSL).  Is there a way to try to get more out of the API via PSL?  (A: Yes, get there soon)

Utility:



* We want to improve the taxonomy, still figuring out how.  We asked the IAB to get involved, or whoever ad tech trusts.  Please tell us who is better to come up with a taxonomy.
* We want improved TOpics weighting.  Current algorithm is just counting pages, that's naive!  Would be great to get something like commercial value!  Who can provide that?  I could just get it from Google, but that seems less than fair.  Where else?
* Simulations: It would be great to have a simulator to run proposed changes against.  We don't have revenue data or user data that we can give out.  We can talk about information gain of topics you receive, but that's going to be pretty naive also.

Martin: Current taxonomy is pretty limited, has cultural bias, etc.  Extending taxonomy creates more information that the API might reveal.  Also creates effects that exaggerates the non-uniformity of users in a population  (Josh: No, that was Ads data, not Chrome Sync data.)  So you looked at a large amount of data over a large population.  Data says actual entropy was much lower than the theoretical maximum — which suggest that some topics or combinations of those are highly unique.  Would you have minimum size threshold met?  Simply can't learn from the aggregate information.  With more topics, you increase the change of having highly specialized individuals.  Moreover, users who visit certain sites are a naturally biased sample — most users want to visit a website for its primary interest; website on needlepoint will get few people interested in wrestling, but when you do get them they are skewed small subsets.  Then the number of people who express genuine interest in particular topics to be quite small.  I don't know how to assess the risk of that happening.  When we concentrate on aggregate metrics, we lose ability to reason about that risk.  Don't know if that's come up before.

Josh: General concern that individual users might be highly identifiable even if majority are not.  I think we have to accept that.

Martin: I cannot accept that.

Josh: We are removing the ability to track the great majority of users, which makes tracking users less desirable.

Martin: DP provides strong guarantees about the privacy of individuals.  FOr people who are identifiable somehow, we're providing them with no privacy.  That doesn't fit with my model of protecting privacy at all.

Theo: Which does user get more benefit from?  TOpics or aggregate reporting?  I think there *is* direct user-perceivble value from topics tht affects what you see, rather than aggregate which doesn't let users see any value.

Aram: Value is in user behavior.  Users access sites for free in exchange for ads.  Users turn off cookies, block ads, etc.  I don't think it's worthwhile to compare.  User's perception of value is "I accessed the site for free"

Don: What users prefer about personalization: A lot of the research has shown that there are more than one kind of user — some strong personalizers, some highly creeped out, some in the middle.  Detect the user's personalization preferences?  Hard to generalize.

Ben: Back to Josh's q on Taxonomy.  One possible way to do this is to go to people — get from them what is their mental model for how topics are segmented in the world.  Ads have utility when you find products that you eventually buy.  Instead of asking what topics they are interested in, ask people what they want ads to do for them.  "If you had to see ads, what kind of ads would be relevant?"  See how people map them into concepts..

Theo: You mean ad a research idea, or in the browser?

Ben: Research.  Probably different per locale.

Erik: Martin, you're talking about DP in the measurement use case.  Isn't there some here too?

Martin: It's meaningless

Josh: I disagree!

Martin: Run the math and it's an epsilon of 14 [edit: actually of 10.4].

Josh: Worst-case analysis for you, average-case analysis for me.  But over time, you will identify users no matter what, because the budget resets as time goes by.

Martin: That's different.  Nature of the selection method is very slippery, probabilistic, so some users will be highly identifiable in a relatively short amount of time.  With aggregate measurement proposals, we can baseline the cost of uniquely identifying a person over time.  Cost of doing so over 10 weeks is that you lose the ability to do measurement for everyone else.  Here it's being provided for free, you can process at will, and small subset of time increases.

Erik: One ad tech can start to identify someone without them needing to visit the same two sites repeatedly.

Martin: For any user who visits sites over time, ability to link any pair of them increases with number of sites, increases further with multiple swings as an ad tech.  Part of the reason I'm giving this feedback is that one of the important things we need to give to our users is the guarantee — "If you participate, the worst that can happen is X"  Here I can't do that.

Josh:  Worst case is the same: re-identifying the user across sites.

Martin: THat's the problem we want to give the guarantee that your browsing history is yours.

Alex: Back to taxonomy and utility: Anyone from Brave in the room?  They do this, though not expose as an API.  Collect browsing history, transform into cohorts, make it available as a targeting technique.  Brave will just let you run ads on cohorts, which is effectively this.

Ben: I love somehow shoehorning user control into this somehow.  Maybe at weekly turnover time?  Maybe some people hate this and want to turn it off, some people love it.  Give people an option, "never show this again / choose for me automatically / show me every time".

Theo: I have a deck somewhere.

Daniel: I'm sure a lot of this has been thought about with the 5%.  Maybe this could grow if the noise topic is never chosen?

Josh: Filtering can get rid of the noise for you.  So with filtering, top frame sees any topic means it's probably the random topic.  Filtering causes a bunch of problems, I do think it's important for making forward progress with privacy, but there are concerns with it.  There's also the "filtering as supercookie" problem, use the filtered status as bits of identity, you can set and get.  Not great when designing an API.

Ben: I know that filtering was added in response to various concerns.  Have you considered reverting?

Aram: I do think complaints about FLoC had more to do with other aspects than with the filtering problem.

Artur: I think the privacy regression vs 3p cookies caused by not filtering really is a big problem.  Gives access to information that sites never would have previously had.  Worst for privacy on any dimension is bad.

Ben: I feel like that is a weird baseline.  "What could you have known with 3p cookies?" shouldn't matter, not the goal of the web.  I talked with John Wilander: Imagine a future world with perfect partitioning and no tracking.  Would you want to ship the API in that future world?  Because it has indirect benefits to people and society?"  The strange incentives of filtering are strong too.

Aram: Other consequence, beyond lots of embedded, is problems with trust around reporting organizations.  There was a time where FB only let you had one domain per page, so they had lots of domains take specific verticals that they covered and spin them off into their own domains.  Once FB changed requirements, these spun-off domains got re-united.  Bad to push the ecosystem to do unnatural things.

Alex: We may be over-estimating how much demand will be driven by Topics.  Advertisers try lots of different things.  I don't think Topics will focus a huge amount of money.

Aram: I wrote about why I think topics _will_ become the most trusted.  But publishers will also make decisions based on what they think buyers will want, whether it's true or not.

Don: Topics can be used to exclude audiences too.

Erik: Whether you compare it to 3p cookies or not, there was some analysis to suggest that in extreme cases, FLoC could have leaked browsing history.  If that comes up again, want to avoid leaking browsing history beyond what could have happened.
